{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import copy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset (creating dataframe using pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11858/00-097C-0000-0023-6260-A/README.txt?sequence=1&isAllowed=y\n",
    "\n",
    "The details of the dataset (the knowledge of the values present in the dataset) were found from this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset RNN/hindencorp05.plaintext', sep = '\\t', names = ['source', 'alignment', 'alignment_type', 'english', 'hindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>alignment</th>\n",
       "      <th>alignment_type</th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikiner2013inflected</td>\n",
       "      <td>1-1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Sharaabi</td>\n",
       "      <td>शराबी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>1-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ted</td>\n",
       "      <td>1-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>1-1</td>\n",
       "      <td>manual</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quote-name</td>\n",
       "      <td>1-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>- John Collins</td>\n",
       "      <td>- जॉन कॉलिन्स</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 source alignment alignment_type  \\\n",
       "0  wikiner2013inflected       1-1          1.000   \n",
       "1                   ted       1-1            1.0   \n",
       "2                   ted       1-1            1.0   \n",
       "3             indic2012       1-1         manual   \n",
       "4            quote-name       1-1            1.0   \n",
       "\n",
       "                                             english  \\\n",
       "0                                           Sharaabi   \n",
       "1  politicians do not have permission to do what ...   \n",
       "2         I'd like to tell you about one such child,   \n",
       "3  This percentage is even greater than the perce...   \n",
       "4                                     - John Collins   \n",
       "\n",
       "                                               hindi  \n",
       "0                                              शराबी  \n",
       "1  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "2  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "3   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "4                                      - जॉन कॉलिन्स  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create English to Hindi Translation table from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sharaabi</td>\n",
       "      <td>शराबी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- John Collins</td>\n",
       "      <td>- जॉन कॉलिन्स</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                                           Sharaabi   \n",
       "1  politicians do not have permission to do what ...   \n",
       "2         I'd like to tell you about one such child,   \n",
       "3  This percentage is even greater than the perce...   \n",
       "4                                     - John Collins   \n",
       "\n",
       "                                               hindi  \n",
       "0                                              शराबी  \n",
       "1  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "2  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "3   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "4                                      - जॉन कॉलिन्स  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = df[['english', 'hindi']]\n",
    "translations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273885\n"
     ]
    }
   ],
   "source": [
    "no_of_samples = translations.shape[0]\n",
    "print(no_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate out different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_english = {} # data from different sources as dictionary\n",
    "targets_hindi = {} # corresponding translation to hindi \n",
    "\n",
    "for i in range(0, no_of_samples):\n",
    "    source = df['source'][i]\n",
    "    if source in sources_english:\n",
    "        sources_english[source].append(df['english'][i])\n",
    "        targets_hindi[source].append(df['hindi'][i])\n",
    "    else:\n",
    "        sources_english[source] = []\n",
    "        targets_hindi[source] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikiner2013inflected:  24562\n",
      "ted:  39880\n",
      "indic2012:  37725\n",
      "quote-name:  908\n",
      "launchpad:  66730\n",
      "agro-hunaligned:  293\n",
      "wikiner2013:  20573\n",
      "tides:  49999\n",
      "danielpipes:  6590\n",
      "intercorp:  7495\n",
      "words-word:  2843\n",
      "wikiner2011:  852\n",
      "emille:  8970\n",
      "acl2005:  3440\n",
      "words-example:  1263\n",
      "quote-sent:  1438\n",
      "agro-exact:  307\n"
     ]
    }
   ],
   "source": [
    "for source in sources_english:\n",
    "    print(source + \": \", len(sources_english[source]))\n",
    "    en_file = open('Dataset RNN/en_' + source, 'w', encoding = 'utf-8')\n",
    "    hi_file = open('Dataset RNN/hi_' + source, 'w', encoding = 'utf-8')\n",
    "    for i in range(0, len(sources_english[source])):\n",
    "        en_file.write(str(sources_english[source][i]) + '\\n')\n",
    "        hi_file.write(str(targets_hindi[source][i]) + '\\n')\n",
    "    en_file.close()\n",
    "    hi_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Word Ids\n",
    "For RNN, turn the text into a number. In the function **text_to_ids()**, turn **source_text** and **target_text** from words to ids.\n",
    "\n",
    "Need to add the <EOS> word id at the end of each sentence from **target_text**. This will help the neural network predict when the sentence should end.\n",
    "\n",
    "Get word ids using **source_vocab_to_int** and **target_vocab_to_int**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_text: String that contains all the source text.\n",
    "# target_text: String that contains all the target text.\n",
    "# source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "# target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "# The function returns a tuple of lists (source_id_text, target_id_text)\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \n",
    "    sentences = source_text.split('\\n')\n",
    "    source_id_text = [[source_vocab_to_int[word] for word in line.split()] for line in sentences]\n",
    "    \n",
    "    sentences = target_text.split('\\n')\n",
    "    target_id_text = [[target_vocab_to_int[word] for word in line.split()]+[target_vocab_to_int['<EOS>']] for line in sentences]\n",
    "\n",
    "    return source_id_text, target_id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load dataset from a file\n",
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with io.open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give codes to padding, end of sentences, unknown, and start of sentence\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create lookup tables for dictionary\n",
    "def create_lookup_tables(text):\n",
    "    \n",
    "    # create a set of words by splitting through spaces\n",
    "    vocab = set(text.split())\n",
    "    \n",
    "    # copy the pre-existing codes (shallow copy)\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # starting from the length of codes, assign the numebers to words\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    # reverse mapping from integers to the words\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the text data and save to a file\n",
    "def preprocess_and_save_data(source_path, target_path, text_to_ids, savefilename):\n",
    "    # Preprocess\n",
    "    # load the data from source and target files\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "    \n",
    "    # convert text to lower cases\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    # create lookup tables for source and target\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "    # convert English sentences and corresponding target sentences into their integer ids using table created above\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "    \n",
    "    # Save Data\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open(savefilename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the source with largest no. of sentences, i.e. launchad\n",
    "preprocess_and_save_data(\"Dataset RNN/en_launchpad\", \"Dataset RNN/hi_launchpad\", text_to_ids, \"Dataset RNN/preprocessed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Preprocessed Training data and return \n",
    "def load_preprocess(savefilename):\n",
    "    return pickle.load(open(savefilename, mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess(\"Dataset RNN/preprocessed_data\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
